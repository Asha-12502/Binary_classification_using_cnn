{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yij1pGMHEUFl",
        "outputId": "05c5aaac-3a4e-461f-8675-a45acbe5b785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries\n"
      ],
      "metadata": {
        "id": "T2olUBLXTFGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import linear\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score, confusion_matrix"
      ],
      "metadata": {
        "id": "p1lozpoSEyNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip data\n"
      ],
      "metadata": {
        "id": "IaVdS8zRTSeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip all dataset\n",
        "!unzip '/content/drive/MyDrive/noise_classification_phase16_red_green_blue_and_positive_blury_augmented_with_replaced_data_including_RGB_negative_and_RGB_positive_data_51561_training_3985_validation.zip'"
      ],
      "metadata": {
        "id": "PyjCSa7jEyQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## unzip all_train data\n",
        "!unzip '/content/drive/MyDrive/training_data_all.zip' "
      ],
      "metadata": {
        "id": "zpEFQkDREyTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## unzip all_test data\n",
        "!unzip '/content/drive/MyDrive/testing_data_all.zip'"
      ],
      "metadata": {
        "id": "APbE-N4vEyWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_n = os.listdir('/content/testing_data/negative')\n",
        "len(test_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMSAb7e8o1u6",
        "outputId": "2cf7e305-66df-4194-e0a5-9849639243c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1508"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_p = os.listdir('/content/testing_data/positive')\n",
        "len(test_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRtwQCbyo1xq",
        "outputId": "397f6ca6-d387-4c02-baae-46210ce6874c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "977"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_n = os.listdir('/content/training_data/negative')\n",
        "len(train_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFc7_yngo10C",
        "outputId": "4f3ec612-0b34-41c3-c5d0-fc02c6505d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20742"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_p = os.listdir('/content/training_data/positive')\n",
        "len(train_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gefY334ptXF",
        "outputId": "9651f7b4-01a6-4423-9a93-60da7e6de621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11983"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "g6iIzTxsTalw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove wrong dimensions"
      ],
      "metadata": {
        "id": "4ubNuZCvTmhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## remove wrong dimensions from the negative trainset\n",
        "path = '/content/training_data/negative/'\n",
        "for i in train_n:\n",
        "  a = cv2.imread(path+i)\n",
        "  if a.shape == (256, 320, 3):\n",
        "    os.remove(path +i)\n",
        "print(len(train_n))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooa38fSOrzcI",
        "outputId": "73837571-646e-44ee-e11f-518fd4bd7096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## remove wrong dimensions from the positive trainset\n",
        "path = '/content/training_data/positive/'\n",
        "for i in train_p:\n",
        "  a = cv2.imread(path+i)\n",
        "  if a.shape == (256, 320, 3):\n",
        "    os.remove(path +i)\n",
        "print(len(train_p))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT14LgwNrzes",
        "outputId": "ee88596a-54fc-4e1f-ab70-562be456a594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and validation data\n"
      ],
      "metadata": {
        "id": "wPg_O-GpTxQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms\n",
        "\n",
        "transformer=transforms.Compose([\n",
        "    transforms.ToTensor()  #0-255 to 0-1, numpy to tensors\n",
        "])"
      ],
      "metadata": {
        "id": "6HEr5_RREyYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data with labels\n",
        "\n",
        "# path of training data\n",
        "train_path = '/content/training_data'\n",
        "test_path = '/content/testing_data'\n",
        "\n",
        "# load the train and test data with labels\n",
        "train_data = ImageFolder(train_path ,transform=transformer)\n",
        "test_data = ImageFolder(test_path, transform=transformer)"
      ],
      "metadata": {
        "id": "HqD1zNx3Eybr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set a batch size for train_set and test_set\n",
        "\n",
        "train = DataLoader(train_data, batch_size=16 , shuffle=True)\n",
        "test = DataLoader(test_data , batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "fVGWqAyeEyeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "zI6-GH5ZqnBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "id": "X2I15jiaqnES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "0RWLBMd-T76Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BSET ARCHITECTURE\n"
      ],
      "metadata": {
        "id": "jzIqpHFkUzKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TYPE_2 4TH_MODEL[RE]\n",
        "\n",
        "model = nn.Sequential(\n",
        "    \n",
        "    # convolution layer 1                     # input shape(16,3,320,256)\n",
        "    nn.Conv2d(in_channels =3 ,out_channels= 16, kernel_size = 3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(num_features=16),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size = 2), #(16,16,160,128)\n",
        "\n",
        "    # convolution layer 2                  \n",
        "    nn.Conv2d(in_channels = 16 ,out_channels= 32, kernel_size = 3, stride=1, padding=1),  \n",
        "    nn.BatchNorm2d(num_features=32),\n",
        "    nn.LeakyReLU(), \n",
        "    nn.MaxPool2d(kernel_size = 2),   # output shape after max pooling  (16,32,80,64)\n",
        "\n",
        "    # convolution layer 3                    \n",
        "    nn.Conv2d(in_channels = 32 ,out_channels= 64, kernel_size = 3, stride=1, padding=1),  \n",
        "    nn.BatchNorm2d(num_features=64),\n",
        "    nn.LeakyReLU(),  \n",
        "    nn.MaxPool2d(kernel_size = 2),   # output shape after max pooling  (16,64,40,32)\n",
        "\n",
        "    # convolutional layer 4            \n",
        "    nn.Conv2d(in_channels = 64 ,out_channels= 128, kernel_size = 3, stride=1, padding=1),   \n",
        "    nn.BatchNorm2d(num_features=128),\n",
        "    nn.LeakyReLU(),   \n",
        "    nn.MaxPool2d(kernel_size = 2),   # output shape after max pooling  (16,128,20,16)    \n",
        "    \n",
        "    # convolutional layer 5             \n",
        "    nn.Conv2d(in_channels = 128 ,out_channels= 256, kernel_size = 3, stride=1, padding=1),  \n",
        "    nn.BatchNorm2d(num_features=256),\n",
        "    nn.LeakyReLU(), \n",
        "    nn.MaxPool2d(kernel_size = 2),   # output shape after max pooling  (16,256,10,8)\n",
        "    # flatten layer\n",
        "    nn.Flatten(),\n",
        "    \n",
        "    # dropout\n",
        "    nn.Dropout(p=0.2),\n",
        "\n",
        "    # fully connected layer\n",
        "    nn.Linear(in_features=256*10*8,out_features=250),\n",
        "    nn.LeakyReLU(), \n",
        "    nn.Dropout(p=0.2),\n",
        "    nn.Linear(in_features=250,out_features=50),\n",
        "    nn.LeakyReLU(), \n",
        "    nn.Dropout(p=0.2),\n",
        "    nn.Linear(in_features=50,out_features=1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "qwixpz8gURhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer and loss function\n",
        "\n",
        "lossfn = nn.BCELoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "D5U0GjrEUsWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable GPU and model summary"
      ],
      "metadata": {
        "id": "RtFGnF9CUNHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable GPU\n",
        "\n",
        "device = 'cuda'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "xyIJKlvoEyng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "print(summary(model, (3,320,256)))"
      ],
      "metadata": {
        "id": "f4QXLx9-Eyqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training and validation"
      ],
      "metadata": {
        "id": "sk016MgZUYbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save checkpoint if a new best is achieved\n",
        "# Save checkpoint if test loss is minimum\n",
        "\n",
        "# Save checkpoint if a new best is achieved\n",
        "def best_save_checkpoint(state, is_best, filename):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    if is_best:\n",
        "        print (\"\")\n",
        "        torch.save(state, filename)  # save checkpoint\n",
        "    else:\n",
        "        print (\"\")\n",
        "\n",
        "# Save checkpoint if test loss is minimum\n",
        "def min_save_checkpoint(state, is_min, filename):\n",
        "    if is_min:\n",
        "        print (\"\")\n",
        "        torch.save(state, filename)  # save checkpoint\n",
        "    else:\n",
        "        print (\"\")\n",
        "      "
      ],
      "metadata": {
        "id": "h3AYrRWWEytR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final code with graph\n",
        "# save best accuracy \n",
        "# save minimum loss\n",
        "# save train test accuracy and loss\n",
        "# code for training and evalution\n",
        "# code for plot between training and testing accuracy\n",
        "# code for plot between training and testing loss\n",
        "\n",
        "def train_model(model,lossfn ,optimizer,epochs=100):\n",
        "  min_loss = 15.00\n",
        "  best_acc = 0.0\n",
        "  inter = 5\n",
        "  training_acc = []\n",
        "  training_loss = []\n",
        "  testing_acc =[]\n",
        "  testing_loss = []\n",
        "  ## empty dict for appending acc and loss\n",
        "  dic ={'epoch': [] ,'training_acc': [], 'training_loss': [],'testing_acc':\n",
        "                    [],'testing_loss':[]}\n",
        "  \n",
        "  for epoch in range(1,epochs+1):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for img ,label in train:\n",
        "      img = img.cuda()\n",
        "      label = label.cuda()\n",
        "\n",
        "      pred = model(img)\n",
        "      label = label.unsqueeze(1)\n",
        "      label = label.float()\n",
        "\n",
        "      loss = lossfn(pred, label)\n",
        "           \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "         \n",
        "      train_loss += loss.item()\n",
        "      train_acc += ((pred > 0.5) == label).sum().item()\n",
        "      \n",
        "\n",
        "    training_acc.append(train_acc/len(train_data))\n",
        "    training_loss.append(train_loss/len(train))\n",
        "\n",
        "    # append per epoch train acc and loss\n",
        "    dic['training_acc'].append(train_acc/len(train_data))\n",
        "    dic['training_loss'].append(train_loss/len(train))\n",
        "    dic['epoch'].append(epoch)\n",
        "    # dict to dataframe\n",
        "    df = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    df = df.transpose()\n",
        "    # convert df to csv and save csv\n",
        "    df.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/save_acc_and_loss.csv',index=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for img, label in test:\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        pred = model(img)\n",
        "        label = label.unsqueeze(1)\n",
        "        label = label.float()\n",
        "        loss = lossfn(pred,label)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        test_acc += ((pred > 0.5) == label).sum().item()\n",
        "        \n",
        "\n",
        "    testing_acc.append(test_acc/len(test_data))\n",
        "    testing_loss.append(test_loss/len(test))\n",
        "\n",
        "    # append per epoch test acc and loss\n",
        "    dic['testing_acc'].append(test_acc/len(test_data))\n",
        "    dic['testing_loss'].append(test_loss/len(test))\n",
        "    # dict to dataframe\n",
        "    df = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    df = df.transpose()\n",
        "    # convert df to csv and save csv\n",
        "    df.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/save_acc_and_loss.csv',index=False)\n",
        "\n",
        "\n",
        "    # plot training and testing accuracy\n",
        "    plt.title('training and testing accuracy VS epochs')\n",
        "    plt.figure()\n",
        "    plt.plot(range(epoch), training_acc, label = 'training accuracy' )\n",
        "    plt.plot(range(epoch), testing_acc, label = 'testing accuracy')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training VS testing accuracy')\n",
        "    plt.savefig('/content/training and testing accuracy graph_'+ str(epoch)+'.jpg')\n",
        "\n",
        "    # plot training and testing loss\n",
        "    plt.title('training and testing loss VS epochs')\n",
        "    plt.figure()\n",
        "    plt.plot(range(epoch), training_loss, label = 'training loss')\n",
        "    plt.plot(range(epoch), testing_loss, label = 'testing loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training VS testing loss')\n",
        "    plt.savefig('/content/training and testing loss graph_'+ str(epoch)+'.jpg')\n",
        "    \n",
        "\n",
        "    ### save best test accuracy\n",
        "    # get a accuracy of each epoch\n",
        "    valid_acc_1 = test_acc/len(test_data)    \n",
        "    # get boolian type base on current acc grater than the previous one\n",
        "    # for make if condition true and false\n",
        "    is_best = bool(valid_acc_1 > best_acc)\n",
        "    # save previous acc in variable named best accuracy \n",
        "    best_acc = (max(valid_acc_1 , best_acc))\n",
        "\n",
        "    #### save minimum test loss\n",
        "    # get a loss of each epoch\n",
        "    test_loss_1 = test_loss/len(test)\n",
        "    # get boolian type base on current loss less than the previous one\n",
        "    # for make if condition true and false\n",
        "    is_min = bool(test_loss_1 < min_loss)\n",
        "    # save previous acc in variable named best accuracy \n",
        "    min_loss = test_loss_1\n",
        "\n",
        "    # print matrix\n",
        "    print('epoch {} , training acc {:.4f} , testing acc {:.4f}, training loss {:.4f} , testing loss {:.4f}'.format(\n",
        "        epoch , 100*train_acc/len(train_data) , 100*test_acc/len(test_data), 100*train_loss/len(train), 100*test_loss/len(test)\n",
        "    ))\n",
        "    \n",
        "    #### call save_checkpoint function \n",
        "    # save best test accuracy\n",
        "    best_save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_accuracy': best_acc\n",
        "    }, is_best,'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_epoch_'+str(epoch)+'_best_test_accuracy_'+str(best_acc)+'.pth' ) \n",
        "    \n",
        "    #### call min_save_checkpoint\n",
        "    # save minimum test loss \n",
        "    min_save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'min_test_loss': min_loss\n",
        "    }, is_min,'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_epoch_'+str(epoch)+'_min_test_loss_'+str(min_loss)+'.pth' ) \n",
        "\n",
        "    #### save checkpoints at a interval\n",
        "    if epoch % inter == 0:\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict()\n",
        "      },'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_at_interval_epoch_'+str(epoch)+'.pth' ) \n",
        "\n",
        "    # plot training and testing accuracy\n",
        "  plt.title('training and testing accuracy VS epochs')\n",
        "  plt.plot(range(epochs), training_acc , label = 'training accuracy')\n",
        "  plt.plot(range(epochs), testing_acc , label = 'testing accuracy')\n",
        "  plt.legend()\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('training VS testing accuracy')\n",
        "  plt.savefig('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/training and testing accuracy graph.jpg')\n",
        "  plt.show()\n",
        "\n",
        "  # plot training and testing loss\n",
        "  plt.title('training and testing loss VS epochs')\n",
        "  plt.plot(range(epochs), training_loss , label = 'training loss')\n",
        "  plt.plot(range(epochs), testing_loss , label = 'testing loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('training VS testing loss')\n",
        "  plt.savefig('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/training and testing loss graph.jpg')\n",
        "  plt.show()\n",
        " "
      ],
      "metadata": {
        "id": "muGF_bfYEywy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the train model function\n",
        "train_model(model,lossfn = nn.BCELoss() , optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4))"
      ],
      "metadata": {
        "id": "4rP2WilbGJOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot accuracy and loss graph on both training and validation"
      ],
      "metadata": {
        "id": "2JxylezBVCWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## plot and save graph of saved acc and loss\n",
        "\n",
        "# read csv file\n",
        "data = pd.read_csv('/content/drive/MyDrive/new_training/save_acc_and_loss_new.csv')\n",
        "\n",
        "# plot training and testing accuracy\n",
        "plt.title('training and testing accuracy VS epochs')\n",
        "plt.plot(range(1,38),data['training_acc'] , label = 'training accuracy')\n",
        "plt.plot(range(1,38), data['testing_acc'] , label = 'testing accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('training VS testing accuracy')\n",
        "plt.savefig('/content/drive/MyDrive/new_training/training and testing accuracy graph.jpg')\n",
        "plt.show()\n",
        "\n",
        "# plot training and testing loss\n",
        "plt.title('training and testing loss VS epochs')\n",
        "plt.plot(range(1,38), data['training_loss'] , label = 'training loss')\n",
        "plt.plot(range(1,38), data['testing_loss'] , label = 'testing loss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('training VS testing loss')\n",
        "plt.savefig('/content/drive/MyDrive/new_training/training and testing loss graph.jpg')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "PkwdSsN0GJRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resuming training"
      ],
      "metadata": {
        "id": "KXs7L8eAVOAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load last epoch for retraining"
      ],
      "metadata": {
        "id": "0X7O3a6yVl0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for loading model's weights\n",
        "\n",
        "def load_model(filepath):\n",
        "  checkpoints = torch.load(filepath)\n",
        "  model.load_state_dict(checkpoints['state_dict'])\n",
        "  print(\"=> loaded checkpoint '{}')\".format(filepath))"
      ],
      "metadata": {
        "id": "DHJD-iQGGJUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the load_model\n",
        "load_model('/content/drive/MyDrive/model_at_interval_epoch_50_.pth')"
      ],
      "metadata": {
        "id": "nPt8G__GGJXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model, loss function and optimizer"
      ],
      "metadata": {
        "id": "5YXAki_0VzWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### for resuming training first intialize model, loss function and optimizer\n",
        "model = model()\n",
        "# optimizer and loss fuction\n",
        "lossfn = nn.BCELoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "l5ygQB9kHdRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resuming training"
      ],
      "metadata": {
        "id": "l-ycqzgTWJjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(model,lossfn ,optimizer,epochs=100):\n",
        "  min_loss = 15.00\n",
        "  best_acc = 0.0\n",
        "  inter = 5\n",
        "  training_acc = []\n",
        "  training_loss = []\n",
        "  testing_acc =[]\n",
        "  testing_loss = []\n",
        "  ## empty dict for appending acc and loss\n",
        "  dic ={'epoch': [] ,'training_acc': [], 'training_loss': [],'testing_acc':\n",
        "                    [],'testing_loss':[]}\n",
        "  \n",
        "  for epoch in range(1,epochs+1):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for img ,label in train:\n",
        "      img = img.cuda()\n",
        "      label = label.cuda()\n",
        "\n",
        "      pred = model(img)\n",
        "      label = label.unsqueeze(1)\n",
        "      label = label.float()\n",
        "\n",
        "      loss = lossfn(pred, label)\n",
        "           \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "         \n",
        "      train_loss += loss.item()\n",
        "      train_acc += ((pred > 0.5) == label).sum().item()\n",
        "      \n",
        "\n",
        "    training_acc.append(train_acc/len(train_data))\n",
        "    training_loss.append(train_loss/len(train))\n",
        "\n",
        "    # append per epoch train acc and loss\n",
        "    dic['training_acc'].append(train_acc/len(train_data))\n",
        "    dic['training_loss'].append(train_loss/len(train))\n",
        "    dic['epoch'].append(epoch)\n",
        "    # dict to dataframe\n",
        "    df = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    df = df.transpose()\n",
        "    # convert df to csv and save csv\n",
        "    df.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/save_acc_and_loss.csv',index=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for img, label in test:\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        pred = model(img)\n",
        "        label = label.unsqueeze(1)\n",
        "        label = label.float()\n",
        "        loss = lossfn(pred,label)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        test_acc += ((pred > 0.5) == label).sum().item()\n",
        "        \n",
        "\n",
        "    testing_acc.append(test_acc/len(test_data))\n",
        "    testing_loss.append(test_loss/len(test))\n",
        "\n",
        "    # append per epoch test acc and loss\n",
        "    dic['testing_acc'].append(test_acc/len(test_data))\n",
        "    dic['testing_loss'].append(test_loss/len(test))\n",
        "    # dict to dataframe\n",
        "    df = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    df = df.transpose()\n",
        "    # convert df to csv and save csv\n",
        "    df.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/save_acc_and_loss.csv',index=False)\n",
        "\n",
        "\n",
        "    # plot training and testing accuracy\n",
        "    plt.title('training and testing accuracy VS epochs')\n",
        "    plt.figure()\n",
        "    plt.plot(range(epoch), training_acc, label = 'training accuracy' )\n",
        "    plt.plot(range(epoch), testing_acc, label = 'testing accuracy')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training VS testing accuracy')\n",
        "    plt.savefig('/content/training and testing accuracy graph_'+ str(epoch)+'.jpg')\n",
        "\n",
        "    # plot training and testing loss\n",
        "    plt.title('training and testing loss VS epochs')\n",
        "    plt.figure()\n",
        "    plt.plot(range(epoch), training_loss, label = 'training loss')\n",
        "    plt.plot(range(epoch), testing_loss, label = 'testing loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training VS testing loss')\n",
        "    plt.savefig('/content/training and testing loss graph_'+ str(epoch)+'.jpg')\n",
        "    \n",
        "\n",
        "    ### save best test accuracy\n",
        "    # get a accuracy of each epoch\n",
        "    valid_acc_1 = test_acc/len(test_data)    \n",
        "    # get boolian type base on current acc grater than the previous one\n",
        "    # for make if condition true and false\n",
        "    is_best = bool(valid_acc_1 > best_acc)\n",
        "    # save previous acc in variable named best accuracy \n",
        "    best_acc = (max(valid_acc_1 , best_acc))\n",
        "\n",
        "    #### save minimum test loss\n",
        "    # get a loss of each epoch\n",
        "    test_loss_1 = test_loss/len(test)\n",
        "    # get boolian type base on current loss less than the previous one\n",
        "    # for make if condition true and false\n",
        "    is_min = bool(test_loss_1 < min_loss)\n",
        "    # save previous acc in variable named best accuracy \n",
        "    min_loss = test_loss_1\n",
        "\n",
        "    # print matrix\n",
        "    print('epoch {} , training acc {:.4f} , testing acc {:.4f}, training loss {:.4f} , testing loss {:.4f}'.format(\n",
        "        epoch , 100*train_acc/len(train_data) , 100*test_acc/len(test_data), 100*train_loss/len(train), 100*test_loss/len(test)\n",
        "    ))\n",
        "    \n",
        "    #### call save_checkpoint function \n",
        "    # save best test accuracy\n",
        "    best_save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_accuracy': best_acc\n",
        "    }, is_best,'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_epoch_'+str(epoch)+'_best_test_accuracy_'+str(best_acc)+'.pth' ) \n",
        "    \n",
        "    #### call min_save_checkpoint\n",
        "    # save minimum test loss \n",
        "    min_save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'min_test_loss': min_loss\n",
        "    }, is_min,'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_epoch_'+str(epoch)+'_min_test_loss_'+str(min_loss)+'.pth' ) \n",
        "\n",
        "    #### save checkpoints at a interval\n",
        "    if epoch % inter == 0:\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict()\n",
        "      },'/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/'+'model_at_interval_epoch_'+str(epoch)+'.pth' ) \n",
        "\n",
        "    # plot training and testing accuracy\n",
        "  plt.title('training and testing accuracy VS epochs')\n",
        "  plt.plot(range(epochs), training_acc , label = 'training accuracy')\n",
        "  plt.plot(range(epochs), testing_acc , label = 'testing accuracy')\n",
        "  plt.legend()\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('training VS testing accuracy')\n",
        "  plt.savefig('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/training and testing accuracy graph.jpg')\n",
        "  plt.show()\n",
        "\n",
        "  # plot training and testing loss\n",
        "  plt.title('training and testing loss VS epochs')\n",
        "  plt.plot(range(epochs), training_loss , label = 'training loss')\n",
        "  plt.plot(range(epochs), testing_loss , label = 'testing loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('training VS testing loss')\n",
        "  plt.savefig('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/training and testing loss graph.jpg')\n",
        "  plt.show()\n",
        " "
      ],
      "metadata": {
        "id": "f6j6AFpgEy4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the train model function\n",
        "train_model(model,lossfn = nn.BCELoss() , optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4))"
      ],
      "metadata": {
        "id": "aSGvbQKlEy6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on validation data"
      ],
      "metadata": {
        "id": "CAF7Dnj0WTm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the best epoch model "
      ],
      "metadata": {
        "id": "blqh1NKEWjVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on this model\n",
        "# function for loading best epoch model's weights\n",
        "\n",
        "def load_model(filepath):\n",
        "  checkpoints = torch.load(filepath)\n",
        "  model.load_state_dict(checkpoints['state_dict'])\n",
        "  print(\"=> loaded checkpoint '{}' (best epoch acco to accu is {})\".format(filepath, checkpoints['epoch']))"
      ],
      "metadata": {
        "id": "YT-PR1kXHX4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable GPU\n",
        "\n",
        "device = 'cuda'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "tIvYWYEmvmhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the load_model\n",
        "load_model('/content/drive/MyDrive/model_epoch_65_min_test_loss_0.030830490661826797_second.pth')"
      ],
      "metadata": {
        "id": "pgcjOospHX6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "eyM2r8ZmWyYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of negative images of validation set\n",
        "negative = os.listdir('/content/all_pred_tiles/negative')\n",
        "source = '/content/all_pred_tiles/negative/'\n",
        "\n",
        "# negative pred\n",
        "dic = {'images': [], 'true_label': [], 'prediction': []}\n",
        "c = 0\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i in negative:\n",
        "    dic['images'].append(i)\n",
        "    dic['true_label'].append(c)\n",
        "    image = cv2.imread(source+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    image = transform(image)\n",
        "    image = image.cuda()\n",
        "    image = image.unsqueeze(0)\n",
        "    pred = model(image)\n",
        "    if (pred > 0.5):\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    dic['prediction'].append(pred)\n",
        "    dataf = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    dataf = dataf.transpose()\n",
        "    dataf.to_csv('/content/drive/MyDrive/created_data_pred_4th_model/with_negative_pred_65_epoch.csv',index=False)  "
      ],
      "metadata": {
        "id": "tBtJ_OIUv3Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of positive images of validation set\n",
        "positive = os.listdir('/content/all_pred_tiles/positive')\n",
        "source_p = '/content/all_pred_tiles/positive/'\n",
        "\n",
        "# positive pred\n",
        "c = 1\n",
        "dic = {'images': [], 'true_label': [], 'prediction': []}\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i in positive:\n",
        "    dic['images'].append(i)\n",
        "    dic['true_label'].append(c)\n",
        "    image = cv2.imread(source_p+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    image = transform(image)\n",
        "    image = image.cuda()\n",
        "    image = image.unsqueeze(0)\n",
        "    pred = model(image)\n",
        "    if (pred > 0.5):\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    dic['prediction'].append(pred)\n",
        "    dataf = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    dataf = dataf.transpose()\n",
        "    dataf.to_csv('/content/drive/MyDrive/created_data_pred_4th_model/with_positive_pred_65_epoch.csv',index=False)"
      ],
      "metadata": {
        "id": "PS9hY5tagX0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read pred csv\n",
        "df_p = pd.read_csv('/content/drive/MyDrive/created_data_pred_4th_model/with_positive_pred_65_epoch.csv')\n",
        "df_n = pd.read_csv('/content/drive/MyDrive/created_data_pred_4th_model/with_negative_pred_65_epoch.csv')\n",
        "\n",
        "## append dataframe\n",
        "all_pred = df_p.append(df_n)\n",
        "\n",
        "## all_pred convert to csv\n",
        "all_pred.to_csv('/content/drive/MyDrive/created_data_pred_4th_model/all_positive_and_negative_pred_65_epoch.csv', index=False)\n",
        "predi = pd.read_csv('/content/drive/MyDrive/created_data_pred_4th_model/all_positive_and_negative_pred_65_epoch.csv')\n",
        "predi"
      ],
      "metadata": {
        "id": "ihARux3HgX3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalution matrix of validation dataset\n"
      ],
      "metadata": {
        "id": "1wLMkIFvW6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## evulation of epoch 65 model\n",
        "# final best epoch on base architecture model \n",
        "# prediction on validation set\n",
        "\n",
        "y_true =[]\n",
        "y_pred = []\n",
        "for true , pred in zip(predi['true_label'],predi['prediction']):\n",
        "  y_true.append(true)\n",
        "  y_pred.append(pred)\n",
        "  \n",
        "precision = precision_score(y_true,y_pred)\n",
        "print('precison of model: ', 100* precision)\n",
        "\n",
        "recall = recall_score(y_true,y_pred)\n",
        "print('Recall of model: ',100*recall)\n",
        "\n",
        "f1_sc = f1_score(y_true,y_pred)\n",
        "print('F1_score is: ', 100*f1_sc)\n",
        "\n",
        "accu = accuracy_score(y_true,y_pred)\n",
        "print('Accuracy is: ', 100*accu)\n",
        "\n",
        "matrix = confusion_matrix(y_true,y_pred)\n",
        "print('Confusion matrix is: \\n', matrix)"
      ],
      "metadata": {
        "id": "i7bJmfYvglQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on Unseen dataset"
      ],
      "metadata": {
        "id": "KdicDfa2XEey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the best epoch"
      ],
      "metadata": {
        "id": "f2VgpBeWZXJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction unseen data\n",
        "# function for loading best epoch model's weights\n",
        "\n",
        "def load_model(filepath):\n",
        "  checkpoints = torch.load(filepath)\n",
        "  model.load_state_dict(checkpoints['state_dict'])\n",
        "  print(\"=> loaded checkpoint '{}' (best epoch acco to accu is {})\".format(filepath, checkpoints['epoch']))"
      ],
      "metadata": {
        "id": "IhTx3yi2Xgqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable GPU\n",
        "\n",
        "device = 'cuda'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Ht5q_TwqnOAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the load_model\n",
        "load_model('/content/drive/MyDrive/model_epoch_65_min_test_loss_0.030830490661826797_second.pth')"
      ],
      "metadata": {
        "id": "9nzqT_CqXgtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "48e3Lev6ZdT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import unseen excel\n",
        "df_unseen = pd.read_excel('/content/noise_classification_phase16_red_green_blue_and_positive_blury_augmented_with_replaced_data_including_RGB_negative_and_RGB_positive_data_51561_training_3985_validation/Positive_and_Negative_Image_List_for_Unseen_Data.xlsx')\n",
        "df_unseen.drop(df_unseen.index[0], inplace=True)\n",
        "df_unseen\n"
      ],
      "metadata": {
        "id": "ka3Yc7q8uSJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on unseendata\n",
        "# prediction on negative image \n",
        "source = '/content/noise_classification_phase16_red_green_blue_and_positive_blury_augmented_with_replaced_data_including_RGB_negative_and_RGB_positive_data_51561_training_3985_validation/unseen_set/Negative_Images/'\n",
        "c = 0\n",
        "dic = {'images': [], 'true_label': [], 'prediction': []}\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i in df_unseen['Negative Images']:\n",
        "    dic['images'].append(i)\n",
        "    dic['true_label'].append(c)\n",
        "    image = cv2.imread(source+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    image = transform(image)\n",
        "    image = image.cuda()\n",
        "    image = image.unsqueeze(0)\n",
        "    pred = model(image)\n",
        "    if (pred > 0.5):\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    dic['prediction'].append(pred)\n",
        "    dataf = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    dataf = dataf.transpose()\n",
        "    dataf.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/with_negative_prediction_file_unseendata_epoch_73.csv',index=False)"
      ],
      "metadata": {
        "id": "MezZTcU1nY_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on positive image \n",
        "# prediction on unseendata\n",
        "source = '/content/noise_classification_phase16_red_green_blue_and_positive_blury_augmented_with_replaced_data_including_RGB_negative_and_RGB_positive_data_51561_training_3985_validation/unseen_set/Positive_Images/'\n",
        "c = 1\n",
        "dic = {'images': [], 'true_label': [], 'prediction': []}\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i in df_unseen['Positive Images']:\n",
        "    dic['images'].append(i)\n",
        "    dic['true_label'].append(c)\n",
        "    image = cv2.imread(source+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    image = transform(image)\n",
        "    image = image.cuda()\n",
        "    image = image.unsqueeze(0)\n",
        "    #print(image.shape)\n",
        "    pred = model(image)\n",
        "    if (pred > 0.5):\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    dic['prediction'].append(pred)\n",
        "    dataf = pd.DataFrame.from_dict(dic, orient='index') \n",
        "    dataf = dataf.transpose()\n",
        "    dataf.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/with_positive_prediction_file_unseendata_epoch_73.csv',index=False)"
      ],
      "metadata": {
        "id": "4G6bsy4Tnf6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen_p = pd.read_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/with_positive_prediction_file_unseendata_epoch_73.csv')\n",
        "df_unseen_n = pd.read_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/with_negative_prediction_file_unseendata_epoch_73.csv')"
      ],
      "metadata": {
        "id": "oHzZw7zbnm5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_unseen_data = df_unseen_p.append(df_unseen_n)\n",
        "df_all_unseen_data"
      ],
      "metadata": {
        "id": "sTgdK2Uanm8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_unseen_data.to_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/all_unseen_data_predi_epoch_73.csv',index=False)\n",
        "# import unseen data predi csv\n",
        "unseen = pd.read_csv('/content/drive/MyDrive/TYPE_2_4TH_MODEL_RE/all_unseen_data_predi_epoch_73.csv')\n",
        "unseen"
      ],
      "metadata": {
        "id": "fn_2Wqumnm_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalution matrix of unseen data"
      ],
      "metadata": {
        "id": "YcnHquKWZlX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## evulation of epoch 65 model\n",
        "# final best epoch on base architecture model \n",
        "# prediction on UNSEEN set\n",
        "\n",
        "y_true =[]\n",
        "y_pred = []\n",
        "for true , pred in zip(unseen['true_label'],unseen['prediction']):\n",
        "  y_true.append(true)\n",
        "  y_pred.append(pred)\n",
        "  \n",
        "precision = precision_score(y_true,y_pred)\n",
        "print('precison of model: ', 100* precision)\n",
        "\n",
        "recall = recall_score(y_true,y_pred)\n",
        "print('Recall of model: ',100*recall)\n",
        "\n",
        "f1_sc = f1_score(y_true,y_pred)\n",
        "print('F1_score is: ', 100*f1_sc)\n",
        "\n",
        "accu = accuracy_score(y_true,y_pred)\n",
        "print('Accuracy is: ', 100*accu)\n",
        "\n",
        "matrix = confusion_matrix(y_true,y_pred)\n",
        "print('Confusion matrix is: \\n', matrix)"
      ],
      "metadata": {
        "id": "0Nuo59HSntQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}